{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93de42f",
   "metadata": {},
   "source": [
    "## Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf982bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import log\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import unicodedata\n",
    "from random import randint\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70da28",
   "metadata": {},
   "source": [
    "## Data Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d0673",
   "metadata": {},
   "source": [
    "#### Get the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('news_summary.csv', encoding='iso-8859-1').reset_index(drop=True)\n",
    "df2 = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ef28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b0146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted columns\n",
    "df1 = df1[['text', 'headlines']]\n",
    "\n",
    "# Remove any rows with missing data\n",
    "df1.dropna(inplace=True)\n",
    "\n",
    "# Concatenate df1 and df2 in df \n",
    "df = pd.concat([df1, df2], axis='rows')\n",
    "del df1, df2\n",
    "\n",
    "# Shuffling the data frame\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f'Dataset size Dataset: {len(df)}')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48fa7c",
   "metadata": {},
   "source": [
    "#### Convert to Lower case  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(str.lower)\n",
    "df.headlines = df.headlines.apply(str.lower)\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3299fba",
   "metadata": {},
   "source": [
    "#### Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e681b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove puncuation from word\n",
    "def rm_punc_from_word(word):\n",
    "    clean_alphabet_list = [alphabet for alphabet in word if alphabet not in string.punctuation]\n",
    "    return ''.join(clean_alphabet_list)\n",
    "\n",
    "# Remove puncuation from text\n",
    "def rm_punc_from_text(text):\n",
    "    clean_word_list = [rm_punc_from_word(word) for word in text]\n",
    "    return ''.join(clean_word_list)\n",
    "\n",
    "# Remove numbers from text\n",
    "def rm_number_from_text(text):\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72eb77f",
   "metadata": {},
   "source": [
    "#### Remove Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed543b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    text = re.sub(\"(__+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(--+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(~~+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\+\\++)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()\n",
    "\n",
    "    text = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n",
    "    text = text.lower()\n",
    "    text = remove_stopwords(text)\n",
    "    text = rm_punc_from_text(text)\n",
    "    text = rm_number_from_text(text)\n",
    "    text = re.sub('–', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "df.text = df.text.apply(clean_text)\n",
    "df.headlines = df.headlines.apply(clean_text)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6436e",
   "metadata": {},
   "source": [
    "#### Word Tokenization and Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "     # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    " \n",
    "    # Stem each word\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        \n",
    "\n",
    "    # Join the preprocessed sentences\n",
    "    preprocessed_sentences = []\n",
    "    preprocessed_sentences.append(' '.join(stemmed_words))\n",
    "    # Join the preprocessed sentences\n",
    "    preprocessed_text = ' '.join(preprocessed_sentences)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply the preprocessing function to the text and headlines columns\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "df['headlines'] = df['headlines'].apply(preprocess_text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e94be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "df.to_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9a9e8",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25823c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 10 most frequent words in the corpus is: \n",
      " [('said', 15367), ('year', 6535), ('ad', 6501), ('india', 5484), ('also', 3405), ('polic', 3159), ('us', 3053), ('minist', 2991), ('govern', 2978), ('old', 2963)]\n",
      "-------------------------------------------------\n",
      "The TF-IDF scores for the first article is: \n",
      " {'major': 0.11445516428587571, 'inflammatori': 0.22473511888715572, 'ace': 0.21695992773981032, 'base': 0.0810684701065436, 'till': 0.11838138163420596, 'centr': 0.10445231358372034, 'drug': 0.3966505224684188, 'among': 0.10607767463070772, 'court': 0.14661977085588, 'indian': 0.0643922222275481, 'submit': 0.1358257973802258, 'allow': 0.10031614243073968, 'proxyvon': 0.2544273429052128, 'relev': 0.1851043872981442, 'ministri': 0.11177559535844202, 'arriv': 0.13499921837016957, 'combin': 0.1543921544156859, 'medicin': 0.32591467157551884, 'delhi': 0.0730783182962899, 'high': 0.09318013373970757, 'dose': 0.21695992773981032, 'wockhardt': 0.2544273429052128, 'health': 0.1202964069522423, 'ask': 0.07754542367917229, 'anti': 0.11900476738909774, 'septemb': 0.1092121831878127, 'report': 0.06520448825468639, 'fdc': 0.24346882646985696, 'fix': 0.1384689796309319, 'sell': 0.11973527204823069, 'ban': 0.21397246335865788, 'decis': 0.10710197929532896}\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Define the function for computing the TF-IDF scores\n",
    "def compute_tfidf(corpus):\n",
    "    # Compute the document frequency for each term in the corpus\n",
    "    df_dict = {}\n",
    "    for doc in corpus:\n",
    "        for term in set(doc):\n",
    "            df_dict[term] = df_dict.get(term, 0) + 1  # The dict.get() method returns the value for a given key \n",
    "                                                     # if it exists in the dictionary, or a default value (in this case, 0) \n",
    "                                                     # if it doesn't exist. We can then add 1 to this value to increment the count.\n",
    "    \n",
    "    # Compute the inverse document frequency for each term\n",
    "    N = len(corpus)         # N is the total number of documents in the corpus\n",
    "    # computes  IDF by taking the natural logarithm of N divided by its document frequency.\n",
    "    idf_dict = {term: log(N/df) for term, df in df_dict.items()}      # df(term) is the number of documents in the corpus that contain the term.\n",
    "    \n",
    "    # Compute the term frequency - inverse document frequency for each term in each document\n",
    "    tfidf_matrix = []\n",
    "    for doc in corpus:\n",
    "        tfidf_doc = {}\n",
    "        for term in set(doc):\n",
    "            df = doc.count(term) / len(doc)\n",
    "            idf = idf_dict[term]\n",
    "            tfidf_doc[term] = df * idf\n",
    "        tfidf_matrix.append(tfidf_doc)\n",
    "    \n",
    "    return tfidf_matrix\n",
    "\n",
    "\n",
    "# Preprocess the text data\n",
    "corpus = [word_tokenize(text) for text in df['text']]\n",
    "\n",
    "# Compute the TF-IDF scores for the text data\n",
    "tfidf_matrix = compute_tfidf(corpus)\n",
    "\n",
    "# Print the top 10 most frequent words in the corpus\n",
    "freq_dist = nltk.FreqDist([term for doc in corpus for term in doc])\n",
    "print('The Top 10 most frequent words in the corpus is: \\n',freq_dist.most_common(10))\n",
    "print('-------------------------------------------------')\n",
    "# Print the TF-IDF scores for the first document\n",
    "print('The TF-IDF scores for the first article is: \\n',tfidf_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4ab23",
   "metadata": {},
   "source": [
    "#### PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f10bb791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>headlines</th>\n",
       "      <th>text_pos</th>\n",
       "      <th>headlines_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delhi high court allow indian drug major wockh...</td>\n",
       "      <td>hc allow wockhardt sell drug ban govt till sept</td>\n",
       "      <td>RB JJ NN JJ JJ NN JJ NN NN IN JJ JJ NN NN NN J...</td>\n",
       "      <td>NN VB NN JJ NN NN NN NN VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk ms dhoni slam three consecut fifti odi se...</td>\n",
       "      <td>ms dhoni superstar time great justin langer</td>\n",
       "      <td>NN NN NN VBD CD NN NN IN JJ JJ NN NN JJ NN NN ...</td>\n",
       "      <td>NN NN NN NN JJ NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accord monitor committe overse clean yamuna ab...</td>\n",
       "      <td>delhi slum major caus yamuna pollut panel</td>\n",
       "      <td>NN NN NN JJ JJ NN NN NN NN JJ NN NN VBZ CD JJ ...</td>\n",
       "      <td>NN NN JJ NN NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comedian televis host krushna abhishek said ka...</td>\n",
       "      <td>kapil sharma give one liner perform krushna</td>\n",
       "      <td>JJ NN NN NN NN VBD JJ JJ VB CD JJR NN NN VBD J...</td>\n",
       "      <td>NNS VBD JJ CD NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>border road organis bro reportedli construct t...</td>\n",
       "      <td>tunnel propos arunach cut distanc china border</td>\n",
       "      <td>NN NN NN NN JJ NN CD NN NN JJ NN NN NN NN NN N...</td>\n",
       "      <td>NN NN NN NN NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24508</th>\n",
       "      <td>former pm manmohan singh monday alleg central ...</td>\n",
       "      <td>bjp govt take nation wrong path ex pm manmohan...</td>\n",
       "      <td>JJ NN NN NN NN IN JJ NNS VBP NN IN JJ NN VBD J...</td>\n",
       "      <td>NN NNS VBP NN JJ NN NN NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24509</th>\n",
       "      <td>photograph actress kareena kapoor celebr chris...</td>\n",
       "      <td>pic kareena celebr christma eve share onlin</td>\n",
       "      <td>NN NN VBD NNP NN NN VBP NN NN NN FW JJ NN NNS ...</td>\n",
       "      <td>NN NN NN NN VBP NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24510</th>\n",
       "      <td>sridevi said alway pressur actor look good eve...</td>\n",
       "      <td>star look best even tackl hell within sridevi</td>\n",
       "      <td>NN VBD RB JJ NN VBP JJ RB VBP NN NN RBR NN JJS...</td>\n",
       "      <td>NN NN RBS RB VBP NN IN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24511</th>\n",
       "      <td>comment sexual harass alleg made tanushre dutt...</td>\n",
       "      <td>silenc voic pooja tanushre row</td>\n",
       "      <td>NN JJ NN NN VBD JJ NN NN NN NN NN VBD VBP JJ N...</td>\n",
       "      <td>NN NN NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24512</th>\n",
       "      <td>call violenc bulandshahr kill one policeman yo...</td>\n",
       "      <td>unfair give polit colour bulandshahr clash shah</td>\n",
       "      <td>NN NN NN VB CD NN NN JJ NN JJ NN NN NN VBD MD ...</td>\n",
       "      <td>JJ JJ NN NN NN NN NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24513 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      delhi high court allow indian drug major wockh...   \n",
       "1      talk ms dhoni slam three consecut fifti odi se...   \n",
       "2      accord monitor committe overse clean yamuna ab...   \n",
       "3      comedian televis host krushna abhishek said ka...   \n",
       "4      border road organis bro reportedli construct t...   \n",
       "...                                                  ...   \n",
       "24508  former pm manmohan singh monday alleg central ...   \n",
       "24509  photograph actress kareena kapoor celebr chris...   \n",
       "24510  sridevi said alway pressur actor look good eve...   \n",
       "24511  comment sexual harass alleg made tanushre dutt...   \n",
       "24512  call violenc bulandshahr kill one policeman yo...   \n",
       "\n",
       "                                               headlines  \\\n",
       "0        hc allow wockhardt sell drug ban govt till sept   \n",
       "1            ms dhoni superstar time great justin langer   \n",
       "2              delhi slum major caus yamuna pollut panel   \n",
       "3            kapil sharma give one liner perform krushna   \n",
       "4         tunnel propos arunach cut distanc china border   \n",
       "...                                                  ...   \n",
       "24508  bjp govt take nation wrong path ex pm manmohan...   \n",
       "24509        pic kareena celebr christma eve share onlin   \n",
       "24510      star look best even tackl hell within sridevi   \n",
       "24511                     silenc voic pooja tanushre row   \n",
       "24512    unfair give polit colour bulandshahr clash shah   \n",
       "\n",
       "                                                text_pos  \\\n",
       "0      RB JJ NN JJ JJ NN JJ NN NN IN JJ JJ NN NN NN J...   \n",
       "1      NN NN NN VBD CD NN NN IN JJ JJ NN NN JJ NN NN ...   \n",
       "2      NN NN NN JJ JJ NN NN NN NN JJ NN NN VBZ CD JJ ...   \n",
       "3      JJ NN NN NN NN VBD JJ JJ VB CD JJR NN NN VBD J...   \n",
       "4      NN NN NN NN JJ NN CD NN NN JJ NN NN NN NN NN N...   \n",
       "...                                                  ...   \n",
       "24508  JJ NN NN NN NN IN JJ NNS VBP NN IN JJ NN VBD J...   \n",
       "24509  NN NN VBD NNP NN NN VBP NN NN NN FW JJ NN NNS ...   \n",
       "24510  NN VBD RB JJ NN VBP JJ RB VBP NN NN RBR NN JJS...   \n",
       "24511  NN JJ NN NN VBD JJ NN NN NN NN NN VBD VBP JJ N...   \n",
       "24512  NN NN NN VB CD NN NN JJ NN JJ NN NN NN VBD MD ...   \n",
       "\n",
       "                         headlines_pos  \n",
       "0          NN VB NN JJ NN NN NN NN VBD  \n",
       "1                 NN NN NN NN JJ NN NN  \n",
       "2                 NN NN JJ NN NN NN NN  \n",
       "3               NNS VBD JJ CD NN NN NN  \n",
       "4                 NN NN NN NN NN NN NN  \n",
       "...                                ...  \n",
       "24508  NN NNS VBP NN JJ NN NN NN NN NN  \n",
       "24509            NN NN NN NN VBP NN NN  \n",
       "24510        NN NN RBS RB VBP NN IN NN  \n",
       "24511                   NN NN NN NN NN  \n",
       "24512             JJ JJ NN NN NN NN NN  \n",
       "\n",
       "[24513 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Define function for PoS tagging\n",
    "def pos_tagging(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Perform PoS tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # Extract the POS tags\n",
    "    pos_tags_only = [tag[1] for tag in pos_tags]\n",
    "    # Join the PoS tags into a string\n",
    "    pos_tags_string = \" \".join(pos_tags_only)\n",
    "    return pos_tags_string\n",
    "\n",
    "# Apply PoS tagging to the text and headlines columns\n",
    "df['text_pos'] = df['text'].apply(pos_tagging)\n",
    "df['headlines_pos'] = df['headlines'].apply(pos_tagging)\n",
    "\n",
    "# Save the POS-tagged dataset\n",
    "df.to_csv('news_summ_pos.csv', index=False)\n",
    "\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7543790f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>headlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>delhi high court allow indian drug major wockh...</td>\n",
       "      <td>hc allow wockhardt sell drug ban govt till sept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>talk ms dhoni slam three consecut fifti odi se...</td>\n",
       "      <td>ms dhoni superstar time great justin langer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>accord monitor committe overse clean yamuna ab...</td>\n",
       "      <td>delhi slum major caus yamuna pollut panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>comedian televis host krushna abhishek said ka...</td>\n",
       "      <td>kapil sharma give one liner perform krushna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>border road organis bro reportedli construct t...</td>\n",
       "      <td>tunnel propos arunach cut distanc china border</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24508</th>\n",
       "      <td>24508</td>\n",
       "      <td>former pm manmohan singh monday alleg central ...</td>\n",
       "      <td>bjp govt take nation wrong path ex pm manmohan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24509</th>\n",
       "      <td>24509</td>\n",
       "      <td>photograph actress kareena kapoor celebr chris...</td>\n",
       "      <td>pic kareena celebr christma eve share onlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24510</th>\n",
       "      <td>24510</td>\n",
       "      <td>sridevi said alway pressur actor look good eve...</td>\n",
       "      <td>star look best even tackl hell within sridevi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24511</th>\n",
       "      <td>24511</td>\n",
       "      <td>comment sexual harass alleg made tanushre dutt...</td>\n",
       "      <td>silenc voic pooja tanushre row</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24512</th>\n",
       "      <td>24512</td>\n",
       "      <td>call violenc bulandshahr kill one policeman yo...</td>\n",
       "      <td>unfair give polit colour bulandshahr clash shah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24513 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text  \\\n",
       "0               0  delhi high court allow indian drug major wockh...   \n",
       "1               1  talk ms dhoni slam three consecut fifti odi se...   \n",
       "2               2  accord monitor committe overse clean yamuna ab...   \n",
       "3               3  comedian televis host krushna abhishek said ka...   \n",
       "4               4  border road organis bro reportedli construct t...   \n",
       "...           ...                                                ...   \n",
       "24508       24508  former pm manmohan singh monday alleg central ...   \n",
       "24509       24509  photograph actress kareena kapoor celebr chris...   \n",
       "24510       24510  sridevi said alway pressur actor look good eve...   \n",
       "24511       24511  comment sexual harass alleg made tanushre dutt...   \n",
       "24512       24512  call violenc bulandshahr kill one policeman yo...   \n",
       "\n",
       "                                               headlines  \n",
       "0        hc allow wockhardt sell drug ban govt till sept  \n",
       "1            ms dhoni superstar time great justin langer  \n",
       "2              delhi slum major caus yamuna pollut panel  \n",
       "3            kapil sharma give one liner perform krushna  \n",
       "4         tunnel propos arunach cut distanc china border  \n",
       "...                                                  ...  \n",
       "24508  bjp govt take nation wrong path ex pm manmohan...  \n",
       "24509        pic kareena celebr christma eve share onlin  \n",
       "24510      star look best even tackl hell within sridevi  \n",
       "24511                     silenc voic pooja tanushre row  \n",
       "24512    unfair give polit colour bulandshahr clash shah  \n",
       "\n",
       "[24513 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9da5d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.headlines = df.headlines.apply(lambda x: f'_START_ {x} _END_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27bae209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# بديل ---> NER \n",
    "# تستخدم لتحديد بداية النص ونهايته  \n",
    "# start_token = 'sostok' # start of Summary\n",
    "# end_token = 'eostok'   # end of Summary\n",
    "# df.headlines = df.headlines.apply(lambda x: f'{start_token} {x} {end_token}')\n",
    "# df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e66f336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model to summarize the text between 0-15 words for Summary and 0-100 words for Text\n",
    "max_text_len=100\n",
    "max_summary_len=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a15f6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 23687\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11224</th>\n",
       "      <td>pm narendra modi saturday met european council...</td>\n",
       "      <td>pm eu leader discuss way fight terror g meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22465</th>\n",
       "      <td>ladi bu driver us milwauke rescu less year old...</td>\n",
       "      <td>us ladi bu driver rescu babi roam highway cold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20760</th>\n",
       "      <td>pakistan state run news channel ptv wrote beij...</td>\n",
       "      <td>imran khan govt fire md state run tv beg error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10598</th>\n",
       "      <td>fijian cricket ilikena lasarusa talebulamainei...</td>\n",
       "      <td>cricket longest known surnam histori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12996</th>\n",
       "      <td>video clip show weatherman struggl stand repor...</td>\n",
       "      <td>report stand peopl walk calmli viral hurrican ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "11224  pm narendra modi saturday met european council...   \n",
       "22465  ladi bu driver us milwauke rescu less year old...   \n",
       "20760  pakistan state run news channel ptv wrote beij...   \n",
       "10598  fijian cricket ilikena lasarusa talebulamainei...   \n",
       "12996  video clip show weatherman struggl stand repor...   \n",
       "\n",
       "                                                 summary  \n",
       "11224       pm eu leader discuss way fight terror g meet  \n",
       "22465  us ladi bu driver rescu babi roam highway cold...  \n",
       "20760     imran khan govt fire md state run tv beg error  \n",
       "10598               cricket longest known surnam histori  \n",
       "12996  report stand peopl walk calmli viral hurrican ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function I use to determine the biggest length and the smallest height I can deal with \n",
    "# select the summary and text between their defined max lens respectively\n",
    "def trim_text_and_summary(df, max_text_len, max_summary_len):\n",
    "    cleaned_text = np.array(df['text'])\n",
    "    cleaned_summary = np.array(df['headlines'])\n",
    "\n",
    "    short_text = []\n",
    "    short_summary = []\n",
    "\n",
    "    for i in range(len(cleaned_text)):\n",
    "        if len(cleaned_text[i].split()) <= max_text_len and len(\n",
    "            cleaned_summary[i].split()\n",
    "        ) <= max_summary_len:\n",
    "            short_text.append(cleaned_text[i])\n",
    "            short_summary.append(cleaned_summary[i])\n",
    "        # change headlines to summary  \n",
    "    df = pd.DataFrame({'text': short_text, 'summary': short_summary})\n",
    "    return df\n",
    "\n",
    "\n",
    "df = trim_text_and_summary(df, max_text_len, max_summary_len)\n",
    "print(f'Dataset size: {len(df)}')\n",
    "df.sample(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdd7f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2b48b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# تستخدم لحذف الكلمات الشاذة \n",
    "# rare word analysis\n",
    "def get_rare_word_percent(tokenizer, threshold):\n",
    "    # threshold: if the word's occurrence is less than this then it's rare word\n",
    "\n",
    "    count = 0\n",
    "    total_count = 0\n",
    "    frequency = 0\n",
    "    total_frequency = 0\n",
    "\n",
    "    for key, value in tokenizer.word_counts.items():\n",
    "        total_count += 1\n",
    "        total_frequency += value\n",
    "        if value < threshold:\n",
    "            count += 1\n",
    "            frequency += value\n",
    "\n",
    "    return {\n",
    "        'percent': round((count / total_count) * 100, 2),\n",
    "        'total_coverage': round(frequency / total_frequency * 100, 2),\n",
    "        'count': count,\n",
    "        'total_count': total_count\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186bd62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cebefffa",
   "metadata": {},
   "source": [
    "### Building Our Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba14bb",
   "metadata": {},
   "source": [
    "#### Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7382ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split( np.array(df['text']), np.array(df['summary']), \n",
    "    test_size=0.2,\n",
    "    random_state=1,\n",
    "    shuffle=True\n",
    ")   # x_val = x_test  AND  y_val = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ea70d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18949,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f55371e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4738,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28569e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets tokenize the text to get the vocab count , you can use Spacy here also\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ea2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_train) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_train    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4960be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "#convert text sequences into integer sequences (i.e one hot encode the text in Y)\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_train) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_train    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1\n",
    "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_train)):\n",
    "    cnt=0\n",
    "    for j in y_train[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_train=np.delete(y_train,ind, axis=0)\n",
    "x_train=np.delete(y_train,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcba066",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "import gensim\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Size of vocabulary from the w2v model = {}\".format(x_voc))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=200\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc19fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09025557",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b90540",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    print(\"Review:\",seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f81830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a1c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4145ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9addf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4e9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0911c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1c4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018212c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e946e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39dfb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22554673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084cbeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4196a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
